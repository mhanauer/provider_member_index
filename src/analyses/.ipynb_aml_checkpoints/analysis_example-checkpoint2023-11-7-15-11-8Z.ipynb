{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%load_ext lab_black"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "The lab_black extension is already loaded. To reload it, use:\n  %reload_ext lab_black\n"
        }
      ],
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Numba --upgrade"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Collecting Numba\n  Downloading numba-0.58.1-cp38-cp38-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.7 MB)\n\u001b[K     |████████████████████████████████| 3.7 MB 6.4 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied, skipping upgrade: numpy<1.27,>=1.22 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from Numba) (1.23.5)\nRequirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.9\" in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from Numba) (6.6.0)\nCollecting llvmlite<0.42,>=0.41.0dev0\n  Downloading llvmlite-0.41.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (43.6 MB)\n\u001b[K     |████████████████████████████████| 43.6 MB 43.6 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied, skipping upgrade: zipp>=0.5 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from importlib-metadata; python_version < \"3.9\"->Numba) (3.12.0)\n\u001b[31mERROR: responsibleai 0.27.0 has requirement ipykernel<=6.8.0, but you'll have ipykernel 6.22.0 which is incompatible.\u001b[0m\n\u001b[31mERROR: responsibleai 0.27.0 has requirement numba<=0.55.2, but you'll have numba 0.58.1 which is incompatible.\u001b[0m\n\u001b[31mERROR: interpret-community 0.29.0 has requirement shap<=0.41.0,>=0.20.0, but you'll have shap 0.43.0 which is incompatible.\u001b[0m\n\u001b[31mERROR: econml 0.14.1 has requirement shap<0.42.0,>=0.38.1, but you'll have shap 0.43.0 which is incompatible.\u001b[0m\n\u001b[31mERROR: azureml-interpret 1.51.0 has requirement numba<=0.55.2, but you'll have numba 0.58.1 which is incompatible.\u001b[0m\n\u001b[31mERROR: azureml-interpret 1.51.0 has requirement numpy<=1.22.3; python_version >= \"3.8\", but you'll have numpy 1.23.5 which is incompatible.\u001b[0m\n\u001b[31mERROR: azureml-interpret 1.51.0 has requirement shap<0.40.0, but you'll have shap 0.43.0 which is incompatible.\u001b[0m\nInstalling collected packages: llvmlite, Numba\n  Attempting uninstall: llvmlite\n    Found existing installation: llvmlite 0.38.1\n    Uninstalling llvmlite-0.38.1:\n      Successfully uninstalled llvmlite-0.38.1\n  Attempting uninstall: Numba\n    Found existing installation: numba 0.55.2\n    Uninstalling numba-0.55.2:\n      Successfully uninstalled numba-0.55.2\nSuccessfully installed Numba-0.58.1 llvmlite-0.41.1\n"
        }
      ],
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyprojroot import here\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "import shap\n",
        "import joblib\n",
        "\n",
        "path_data = here(\"./data\")\n",
        "os.chdir(path_data)\n",
        "\n",
        "data = pd.read_csv(\"data_er_visits.csv\").rename(columns={\"Unnamed: 0\": \"Member ID\"})\n",
        "data.head()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/anaconda/envs/azureml_py38/lib/python3.8/site-packages/xgboost/compat.py:31: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n  from pandas import MultiIndex, Int64Index\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 1,
          "data": {
            "text/plain": "   Member ID  Hospital ID  Age 60+  High Cholesterol  Diabetes  \\\n0          0            1        0                 0         0   \n1          1            1        1                 0         0   \n2          2            3        1                 0         0   \n3          3            1        0                 1         0   \n4          4            3        0                 0         0   \n\n   Preventative Services  ER Visit  \n0                      1         0  \n1                      0         1  \n2                      1         0  \n3                      0         0  \n4                      1         0  ",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Member ID</th>\n      <th>Hospital ID</th>\n      <th>Age 60+</th>\n      <th>High Cholesterol</th>\n      <th>Diabetes</th>\n      <th>Preventative Services</th>\n      <th>ER Visit</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>3</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 1,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1701961807355
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare the data\n",
        "X = data.drop([\"ER Visit\", \"Hospital ID\", \"Member ID\"], axis=1)\n",
        "y = data[\"ER Visit\"]\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Train the XGBoost model\n",
        "model = xgb.XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\")\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions (opxtional, to evaluate model)\n",
        "predictions = model.predict(X_test)"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name 'Int64Index' from 'pandas' (/anaconda/envs/azureml_py38/lib/python3.8/site-packages/pandas/__init__.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[6], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Train the XGBoost model\u001b[39;00m\n\u001b[1;32m     11\u001b[0m model \u001b[38;5;241m=\u001b[39m xgb\u001b[38;5;241m.\u001b[39mXGBClassifier(use_label_encoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, eval_metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogloss\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Make predictions (opxtional, to evaluate model)\u001b[39;00m\n\u001b[1;32m     15\u001b[0m predictions \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.8/site-packages/xgboost/core.py:422\u001b[0m, in \u001b[0;36m_deprecate_positional_args.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[1;32m    421\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 422\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.8/site-packages/xgboost/sklearn.py:903\u001b[0m, in \u001b[0;36mXGBClassifier.fit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[1;32m    900\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_features_count \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    901\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_features_count\n\u001b[0;32m--> 903\u001b[0m train_dmatrix, evals \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wrap_evaluation_matrices\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    904\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbase_margin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_margin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    905\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    906\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight_eval_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight_eval_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    907\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_group\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_transform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabel_transform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    909\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster \u001b[38;5;241m=\u001b[39m train(xgb_options, train_dmatrix,\n\u001b[1;32m    910\u001b[0m                       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_num_boosting_rounds(),\n\u001b[1;32m    911\u001b[0m                       evals\u001b[38;5;241m=\u001b[39mevals,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    914\u001b[0m                       verbose_eval\u001b[38;5;241m=\u001b[39mverbose, xgb_model\u001b[38;5;241m=\u001b[39mxgb_model,\n\u001b[1;32m    915\u001b[0m                       callbacks\u001b[38;5;241m=\u001b[39mcallbacks)\n\u001b[1;32m    917\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective \u001b[38;5;241m=\u001b[39m xgb_options[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobjective\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.8/site-packages/xgboost/sklearn.py:265\u001b[0m, in \u001b[0;36mXGBModel._wrap_evaluation_matrices\u001b[0;34m(self, X, y, group, sample_weight, base_margin, feature_weights, eval_set, sample_weight_eval_set, eval_group, label_transform)\u001b[0m\n\u001b[1;32m    262\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(eval_group) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(eval_set)\n\u001b[1;32m    264\u001b[0m y \u001b[38;5;241m=\u001b[39m label_transform(y)\n\u001b[0;32m--> 265\u001b[0m train_dmatrix \u001b[38;5;241m=\u001b[39m \u001b[43mDMatrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mbase_margin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_margin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mmissing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmissing\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnthread\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    268\u001b[0m train_dmatrix\u001b[38;5;241m.\u001b[39mset_info(feature_weights\u001b[38;5;241m=\u001b[39mfeature_weights, group\u001b[38;5;241m=\u001b[39mgroup)\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m eval_set \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.8/site-packages/xgboost/core.py:500\u001b[0m, in \u001b[0;36mDMatrix.__init__\u001b[0;34m(self, data, label, weight, base_margin, missing, silent, feature_names, feature_types, nthread, enable_categorical)\u001b[0m\n\u001b[1;32m    497\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    499\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dispatch_data_backend\n\u001b[0;32m--> 500\u001b[0m handle, feature_names, feature_types \u001b[38;5;241m=\u001b[39m \u001b[43mdispatch_data_backend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmissing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m    \u001b[49m\u001b[43mthreads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnthread\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    505\u001b[0m \u001b[43m    \u001b[49m\u001b[43menable_categorical\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable_categorical\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    506\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m handle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    507\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle \u001b[38;5;241m=\u001b[39m handle\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.8/site-packages/xgboost/data.py:539\u001b[0m, in \u001b[0;36mdispatch_data_backend\u001b[0;34m(data, missing, threads, feature_names, feature_types, enable_categorical)\u001b[0m\n\u001b[1;32m    537\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _from_tuple(data, missing, feature_names, feature_types)\n\u001b[1;32m    538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_pandas_df(data):\n\u001b[0;32m--> 539\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_from_pandas_df\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menable_categorical\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    540\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mfeature_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_types\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    541\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_pandas_series(data):\n\u001b[1;32m    542\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _from_pandas_series(data, missing, threads, feature_names,\n\u001b[1;32m    543\u001b[0m                                feature_types)\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.8/site-packages/xgboost/data.py:242\u001b[0m, in \u001b[0;36m_from_pandas_df\u001b[0;34m(data, enable_categorical, missing, nthread, feature_names, feature_types)\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_from_pandas_df\u001b[39m(data, enable_categorical, missing, nthread,\n\u001b[1;32m    241\u001b[0m                     feature_names, feature_types):\n\u001b[0;32m--> 242\u001b[0m     data, feature_names, feature_types \u001b[38;5;241m=\u001b[39m \u001b[43m_transform_pandas_df\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menable_categorical\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_types\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    244\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _from_numpy_array(data, missing, nthread, feature_names,\n\u001b[1;32m    245\u001b[0m                              feature_types)\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.8/site-packages/xgboost/data.py:192\u001b[0m, in \u001b[0;36m_transform_pandas_df\u001b[0;34m(data, enable_categorical, feature_names, feature_types, meta, meta_type)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_transform_pandas_df\u001b[39m(data, enable_categorical,\n\u001b[1;32m    190\u001b[0m                          feature_names\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, feature_types\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    191\u001b[0m                          meta\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, meta_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 192\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MultiIndex, Int64Index\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_sparse, is_categorical_dtype\n\u001b[1;32m    195\u001b[0m     data_dtypes \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mdtypes\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'Int64Index' from 'pandas' (/anaconda/envs/azureml_py38/lib/python3.8/site-packages/pandas/__init__.py)"
          ]
        }
      ],
      "execution_count": 6,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1701960267717
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path_production = here(\"./src/production\")\n",
        "os.chdir(path_production)\n",
        "joblib.dump(model, \"model_drivers.joblib\")\n",
        "os.chdir(path_data)"
      ],
      "outputs": [],
      "execution_count": 80,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1701889934029
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming 'data' is your DataFrame and is already defined\n",
        "# ... [Your existing code for data preparation and model training] ...\n",
        "\n",
        "# Create a Tree explainer\n",
        "explainer = shap.Explainer(\n",
        "    model, X_train, model_output=\"probability\", feature_perturbation=\"interventional\"\n",
        ")\n",
        "\n",
        "# Calculate SHAP values - this might take some time for larger datasets\n",
        "shap_values = explainer(data.drop([\"ER Visit\", \"Hospital ID\", \"Member ID\"], axis=1))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": " 94%|=================== | 9383/10000 [00:17<00:01]       \r 99%|===================| 9948/10000 [00:18<00:00]       "
        }
      ],
      "execution_count": 60,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1701889191902
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming shap_values and X have the same order\n",
        "column_mapping = {\n",
        "    i: name\n",
        "    for i, name in enumerate(\n",
        "        data.drop([\"ER Visit\", \"Hospital ID\", \"Member ID\"], axis=1).columns\n",
        "    )\n",
        "}\n",
        "\n",
        "# Rename columns in the DataFrame created from SHAP values\n",
        "data_shap_pd = pd.DataFrame(shap_values.values).rename(columns=column_mapping)"
      ],
      "outputs": [],
      "execution_count": 61,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1701889192005
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_no_id_outcome = data.drop(columns=[\"Member ID\", \"Hospital ID\", \"ER Visit\"])\n",
        "data_percentile = data_no_id_outcome.rank(pct=True)"
      ],
      "outputs": [],
      "execution_count": 62,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1701889192089
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step: Add back hosptial ids and add unique ids"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_percentile_id = pd.concat(\n",
        "    [data[[\"Member ID\", \"Hospital ID\"]], data_percentile], axis=1\n",
        ")\n",
        "\n",
        "data_shap_id = pd.concat([data[[\"Member ID\", \"Hospital ID\"]], data_shap_pd], axis=1)"
      ],
      "outputs": [],
      "execution_count": 63,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1701889192183
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_shap_id.to_csv(\"data_shap_ind.csv\")"
      ],
      "outputs": [],
      "execution_count": 68,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1701889235008
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "def median_shap_for_high_percentiles(percentile_df, shap_df, id_col):\n",
        "    # Initialize a dictionary to store the results\n",
        "    median_shap_values = {id_col: [], \"variable\": [], \"median_shap\": []}\n",
        "\n",
        "    # Iterate over each ID\n",
        "    for id_value in percentile_df[id_col].unique():\n",
        "        # Filter DataFrames for the current ID\n",
        "        percentile_subdf = percentile_df[percentile_df[id_col] == id_value]\n",
        "        shap_subdf = shap_df[shap_df[id_col] == id_value]\n",
        "\n",
        "        # Iterate over each column (except the ID column)\n",
        "        for col in percentile_df.columns:\n",
        "            if col == id_col:\n",
        "                continue\n",
        "\n",
        "            # Calculate the median of the current column in percentile DataFrame\n",
        "            median_value = percentile_subdf[col].median()\n",
        "\n",
        "            # Filter rows where the percentile value is above the median\n",
        "            rows_above_median = percentile_subdf[\n",
        "                percentile_subdf[col] > median_value\n",
        "            ].index\n",
        "\n",
        "            # Calculate the median SHAP value for these rows\n",
        "            median_shap = shap_subdf.loc[rows_above_median, col].median()\n",
        "\n",
        "            # Store the results\n",
        "            median_shap_values[id_col].append(id_value)\n",
        "            median_shap_values[\"variable\"].append(col)\n",
        "            median_shap_values[\"median_shap\"].append(median_shap)\n",
        "\n",
        "    # Convert the dictionary to a DataFrame and return\n",
        "    return pd.DataFrame(median_shap_values)"
      ],
      "outputs": [],
      "execution_count": 69,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1701889283613
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "result_df = (\n",
        "    median_shap_for_high_percentiles(\n",
        "        percentile_df=data_percentile_id.drop(columns=[\"Member ID\"]),\n",
        "        shap_df=data_shap_id,\n",
        "        id_col=\"Hospital ID\",\n",
        "    )\n",
        "    .fillna(-0.20)\n",
        "    .round(2)\n",
        ")"
      ],
      "outputs": [],
      "execution_count": 70,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1701889285899
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "noise = np.random.uniform(-0.07, 0.07, result_df[\"median_shap\"].shape)\n",
        "result_df[\"median_shap\"] = result_df[\"median_shap\"] + noise\n",
        "data_shap_hospital = (\n",
        "    result_df.rename(columns={\"variable\": \"Driver\", \"median_shap\": \"Impact\"})\n",
        "    .round(2)\n",
        "    .query(\"Driver != 'Member ID' \")\n",
        ").reset_index(drop=True)"
      ],
      "outputs": [],
      "execution_count": 72,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1701889307230
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_shap_hospital.to_csv(\"data_shap_hospital.csv\")"
      ],
      "outputs": [],
      "execution_count": 74,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1701889328556
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python38-azureml",
      "language": "python",
      "display_name": "Python 3.8 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "python38-azureml"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}