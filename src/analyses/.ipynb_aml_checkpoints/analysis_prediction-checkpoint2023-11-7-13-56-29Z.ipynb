{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%load_ext lab_black"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyprojroot import here\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "import shap\n",
        "import joblib\n",
        "from joblib import load\n",
        "\n",
        "path_data = here(\"./data\")\n",
        "os.chdir(path_data)\n",
        "\n",
        "data = pd.read_csv(\"data_er_visits.csv\").rename(columns={\"Unnamed: 0\": \"Member ID\"})\n",
        "data_no_member_id = data.drop(columns=[\"Member ID\"])\n",
        "data_predict = data_no_member_id.drop(columns=[\"Hospital ID\", \"ER Visit\"])\n",
        "\n",
        "# Load the model\n",
        "model = joblib.load(\"model_drivers.joblib\")"
      ],
      "outputs": [],
      "execution_count": 12,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1701957369103
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get the actual percentage of members with ER visits in the last 30 days for each hospital\n",
        "\n",
        "Show them their average percentages for all four variables.\n",
        "\n",
        "Add some noise for the features by Hospital ID\n",
        "\n",
        "Then create a function to allow for simulated data with the exact percentage that someone shows"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_no_member_id.groupby(\"Hospital ID\").mean().reset_index().round(2)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 13,
          "data": {
            "text/plain": "   Hospital ID  Age 60+  High Cholesterol  Diabetes  Preventative Services  \\\n0            1     0.29              0.41      0.25                   0.51   \n1            2     0.28              0.40      0.24                   0.50   \n2            3     0.29              0.40      0.24                   0.49   \n\n   ER Visit  \n0      0.28  \n1      0.27  \n2      0.28  ",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Hospital ID</th>\n      <th>Age 60+</th>\n      <th>High Cholesterol</th>\n      <th>Diabetes</th>\n      <th>Preventative Services</th>\n      <th>ER Visit</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>0.29</td>\n      <td>0.41</td>\n      <td>0.25</td>\n      <td>0.51</td>\n      <td>0.28</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>0.28</td>\n      <td>0.40</td>\n      <td>0.24</td>\n      <td>0.50</td>\n      <td>0.27</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>0.29</td>\n      <td>0.40</td>\n      <td>0.24</td>\n      <td>0.49</td>\n      <td>0.28</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 13,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1701957375098
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions (opxtional, to evaluate model)\n",
        "predictions = model.predict(data_predict)"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name 'Int64Index' from 'pandas' (/anaconda/envs/azureml_py38/lib/python3.8/site-packages/pandas/__init__.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[14], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Make predictions (opxtional, to evaluate model)\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_predict\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.8/site-packages/xgboost/sklearn.py:966\u001b[0m, in \u001b[0;36mXGBClassifier.predict\u001b[0;34m(self, data, output_margin, ntree_limit, validate_features, base_margin)\u001b[0m\n\u001b[1;32m    931\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, data, output_margin\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, ntree_limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    932\u001b[0m             validate_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, base_margin\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    933\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    934\u001b[0m \u001b[38;5;124;03m    Predict with `data`.\u001b[39;00m\n\u001b[1;32m    935\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    964\u001b[0m \u001b[38;5;124;03m    prediction : numpy array\u001b[39;00m\n\u001b[1;32m    965\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 966\u001b[0m     test_dmatrix \u001b[38;5;241m=\u001b[39m \u001b[43mDMatrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbase_margin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_margin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    967\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mmissing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmissing\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnthread\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    968\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ntree_limit \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    969\u001b[0m         ntree_limit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest_ntree_limit\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.8/site-packages/xgboost/core.py:500\u001b[0m, in \u001b[0;36mDMatrix.__init__\u001b[0;34m(self, data, label, weight, base_margin, missing, silent, feature_names, feature_types, nthread, enable_categorical)\u001b[0m\n\u001b[1;32m    497\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    499\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dispatch_data_backend\n\u001b[0;32m--> 500\u001b[0m handle, feature_names, feature_types \u001b[38;5;241m=\u001b[39m \u001b[43mdispatch_data_backend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmissing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m    \u001b[49m\u001b[43mthreads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnthread\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    505\u001b[0m \u001b[43m    \u001b[49m\u001b[43menable_categorical\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable_categorical\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    506\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m handle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    507\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle \u001b[38;5;241m=\u001b[39m handle\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.8/site-packages/xgboost/data.py:539\u001b[0m, in \u001b[0;36mdispatch_data_backend\u001b[0;34m(data, missing, threads, feature_names, feature_types, enable_categorical)\u001b[0m\n\u001b[1;32m    537\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _from_tuple(data, missing, feature_names, feature_types)\n\u001b[1;32m    538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_pandas_df(data):\n\u001b[0;32m--> 539\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_from_pandas_df\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menable_categorical\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    540\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mfeature_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_types\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    541\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_pandas_series(data):\n\u001b[1;32m    542\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _from_pandas_series(data, missing, threads, feature_names,\n\u001b[1;32m    543\u001b[0m                                feature_types)\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.8/site-packages/xgboost/data.py:242\u001b[0m, in \u001b[0;36m_from_pandas_df\u001b[0;34m(data, enable_categorical, missing, nthread, feature_names, feature_types)\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_from_pandas_df\u001b[39m(data, enable_categorical, missing, nthread,\n\u001b[1;32m    241\u001b[0m                     feature_names, feature_types):\n\u001b[0;32m--> 242\u001b[0m     data, feature_names, feature_types \u001b[38;5;241m=\u001b[39m \u001b[43m_transform_pandas_df\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menable_categorical\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_types\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    244\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _from_numpy_array(data, missing, nthread, feature_names,\n\u001b[1;32m    245\u001b[0m                              feature_types)\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.8/site-packages/xgboost/data.py:192\u001b[0m, in \u001b[0;36m_transform_pandas_df\u001b[0;34m(data, enable_categorical, feature_names, feature_types, meta, meta_type)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_transform_pandas_df\u001b[39m(data, enable_categorical,\n\u001b[1;32m    190\u001b[0m                          feature_names\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, feature_types\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    191\u001b[0m                          meta\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, meta_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 192\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MultiIndex, Int64Index\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_sparse, is_categorical_dtype\n\u001b[1;32m    195\u001b[0m     data_dtypes \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mdtypes\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'Int64Index' from 'pandas' (/anaconda/envs/azureml_py38/lib/python3.8/site-packages/pandas/__init__.py)"
          ]
        }
      ],
      "execution_count": 14,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1701957384228
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path_production = here(\"./src/production\")\n",
        "os.chdir(path_production)\n",
        "joblib.dump(model, \"model_drivers.joblib\")\n",
        "os.chdir(path_data)"
      ],
      "outputs": [],
      "execution_count": 80,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1701889934029
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming 'data' is your DataFrame and is already defined\n",
        "# ... [Your existing code for data preparation and model training] ...\n",
        "\n",
        "# Create a Tree explainer\n",
        "explainer = shap.Explainer(\n",
        "    model, X_train, model_output=\"probability\", feature_perturbation=\"interventional\"\n",
        ")\n",
        "\n",
        "# Calculate SHAP values - this might take some time for larger datasets\n",
        "shap_values = explainer(data.drop([\"ER Visit\", \"Hospital ID\", \"Member ID\"], axis=1))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": " 94%|=================== | 9383/10000 [00:17<00:01]       \r 99%|===================| 9948/10000 [00:18<00:00]       "
        }
      ],
      "execution_count": 60,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1701889191902
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming shap_values and X have the same order\n",
        "column_mapping = {\n",
        "    i: name\n",
        "    for i, name in enumerate(\n",
        "        data.drop([\"ER Visit\", \"Hospital ID\", \"Member ID\"], axis=1).columns\n",
        "    )\n",
        "}\n",
        "\n",
        "# Rename columns in the DataFrame created from SHAP values\n",
        "data_shap_pd = pd.DataFrame(shap_values.values).rename(columns=column_mapping)"
      ],
      "outputs": [],
      "execution_count": 61,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1701889192005
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_no_id_outcome = data.drop(columns=[\"Member ID\", \"Hospital ID\", \"ER Visit\"])\n",
        "data_percentile = data_no_id_outcome.rank(pct=True)"
      ],
      "outputs": [],
      "execution_count": 62,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1701889192089
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step: Add back hosptial ids and add unique ids"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_percentile_id = pd.concat(\n",
        "    [data[[\"Member ID\", \"Hospital ID\"]], data_percentile], axis=1\n",
        ")\n",
        "\n",
        "data_shap_id = pd.concat([data[[\"Member ID\", \"Hospital ID\"]], data_shap_pd], axis=1)"
      ],
      "outputs": [],
      "execution_count": 63,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1701889192183
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_shap_id.to_csv(\"data_shap_ind.csv\")"
      ],
      "outputs": [],
      "execution_count": 68,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1701889235008
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "def median_shap_for_high_percentiles(percentile_df, shap_df, id_col):\n",
        "    # Initialize a dictionary to store the results\n",
        "    median_shap_values = {id_col: [], \"variable\": [], \"median_shap\": []}\n",
        "\n",
        "    # Iterate over each ID\n",
        "    for id_value in percentile_df[id_col].unique():\n",
        "        # Filter DataFrames for the current ID\n",
        "        percentile_subdf = percentile_df[percentile_df[id_col] == id_value]\n",
        "        shap_subdf = shap_df[shap_df[id_col] == id_value]\n",
        "\n",
        "        # Iterate over each column (except the ID column)\n",
        "        for col in percentile_df.columns:\n",
        "            if col == id_col:\n",
        "                continue\n",
        "\n",
        "            # Calculate the median of the current column in percentile DataFrame\n",
        "            median_value = percentile_subdf[col].median()\n",
        "\n",
        "            # Filter rows where the percentile value is above the median\n",
        "            rows_above_median = percentile_subdf[\n",
        "                percentile_subdf[col] > median_value\n",
        "            ].index\n",
        "\n",
        "            # Calculate the median SHAP value for these rows\n",
        "            median_shap = shap_subdf.loc[rows_above_median, col].median()\n",
        "\n",
        "            # Store the results\n",
        "            median_shap_values[id_col].append(id_value)\n",
        "            median_shap_values[\"variable\"].append(col)\n",
        "            median_shap_values[\"median_shap\"].append(median_shap)\n",
        "\n",
        "    # Convert the dictionary to a DataFrame and return\n",
        "    return pd.DataFrame(median_shap_values)"
      ],
      "outputs": [],
      "execution_count": 69,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1701889283613
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "result_df = (\n",
        "    median_shap_for_high_percentiles(\n",
        "        percentile_df=data_percentile_id.drop(columns=[\"Member ID\"]),\n",
        "        shap_df=data_shap_id,\n",
        "        id_col=\"Hospital ID\",\n",
        "    )\n",
        "    .fillna(-0.20)\n",
        "    .round(2)\n",
        ")"
      ],
      "outputs": [],
      "execution_count": 70,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1701889285899
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "noise = np.random.uniform(-0.07, 0.07, result_df[\"median_shap\"].shape)\n",
        "result_df[\"median_shap\"] = result_df[\"median_shap\"] + noise\n",
        "data_shap_hospital = (\n",
        "    result_df.rename(columns={\"variable\": \"Driver\", \"median_shap\": \"Impact\"})\n",
        "    .round(2)\n",
        "    .query(\"Driver != 'Member ID' \")\n",
        ").reset_index(drop=True)"
      ],
      "outputs": [],
      "execution_count": 72,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1701889307230
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_shap_hospital.to_csv(\"data_shap_hospital.csv\")"
      ],
      "outputs": [],
      "execution_count": 74,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1701889328556
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python38-azureml",
      "language": "python",
      "display_name": "Python 3.8 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "python38-azureml"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}